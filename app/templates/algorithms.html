{% extends "layout.html" %}
{% block body %}
<div class="jumbotron text-justify">
        <center>
                <h3>Algorithms And Experimental Results</h3>
        </center>
        <br>
        <h5>Algorithms</h5>
        <h6>K Nearest Neighbour</h6>
        <br>
        <p>The data set obtained after the preprocessing operations were saved in a csv file. Then, the K Nearest
                Neighbor algorithm was applied on this file.
        </p>
        <p>
                For the K Nearest Neigbour algorithm, KNeighborsClassifier from sklearn.neighbors library and
                train_test_split from sklearn.model_selection library are imported. After these processes, the target
                attribute (Class) that the algorithm will focus on is determined. Then, feature and target arrays were
                created for the data set. Here, the target attribute is removed from the feature array. In the target
                array, there is only the target attribute.
        </p>
        <p>
                After the feature and target arrays are created, the data set is divided into test and train sets, 20
                percent of which is the test set. The ratios of test and train sets can be changed for different
                optimizations of the algorithm.
        </p>
        <p>
                After the test and train sets were separated, the KNN algorithm was run with k as 3. Then the result of
                the KNN algorithm and the results of the test set were compared with the fit function. The algorithm can
                be optimized by giving k different values. For this, the KNN algorithm was run by determining the number
                of k as 3, 5 and 7 and the results were compared.
        </p>
        <p>
                In the last step, the accuracy value of the KNN algorithm was calculated.
                After running the KNN algorithm, it was tested using Cross Validation. For this, cross_val_score is
                imported from the sklearn.model_selection library. Cross Validation values were calculated by entering
                cv value as 10 and k value as 5. The accuracy values produced for the entered cv and k values and the
                average of these accuracy values are shown in Figure 3.2.1.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-1.jpeg" /><br>
                        Figure 3.2.1 KNN Cross Validation Scores
                </center>
        </p>
        <p>
                Finally, for the optimization of the k values, the accuracy values of the algorithm were measured at k
                values between 1 and 41. The result of these measurements is shown in Figure 3.2.2.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-2.jpeg" /><br>
                        Figure 3.2.2 Comparison of different k values
                </center>
        </p>
        <br>
        <h6>Naive Bayes</h6>
        <br>
        <p>The data set obtained after the preprocessing operations were saved in a csv file. Then, the Naive Bayes
                algorithm was applied on this file.
        </p>
        <p>
                For the Naive Bayes algorithm, GaussianNB from sklearn.naive_bayes library and train_test_split from
                sklearn.model_selection library are imported. After these processes, the target attribute (Class) that
                the algorithm will focus on is determined. Then, feature and target arrays were created for the data
                set. Here, the target attribute is removed from the feature array. In the target array, there is only
                the target attribute.
        </p>
        <p>
                After the feature and target arrays are created, the data set is divided into test and train sets, 30
                percent of which is the test set. The ratios of test and train sets can be changed for different
                optimizations of the algorithm.
        </p>
        <p>
                After the test and train sets were separated, the Naive Bayes algorithm was run. Then the result of the
                Naive Bayes algorithm and the results of the test set were compared with the fit function.
        </p>
        <p>
                In the last step, the accuracy value of the Bayes algorithm was calculated and the confusion matrix was
                created. Figure 3.2.3 shows the confusion matrix obtained from the Naive Bayes algorithm.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-3.jpeg" /><br>
                        Figure 3.2.3 Confusion Matrix of Naive Bayes
                </center>

        </p>
        <br>
        <h6> Decision Tree </h6>
        <br>
        <p>In order for the Decision Tree results to give the best accuracy values, a method called gird search should
                be used to find the optimum value among its parameters. Decision Tree has quite different parameters in
                itself. The parameters used to optimize the decision tree in this project are:
                -Criterion: Measure the quality of a split. In this project given ‘gini’ or ‘entropy’.
                -max_depth: the maximum depth of a tree. In this project, None,50,100,150 and 200 are given.
                -Max_features:The number of features to consider when looking for the best split.In this project,
                None,’auto’,’sqrt’ and ‘log2’ are given.
                -Min_samples_split:The minimum number of samples required to split an internal node.In this project,
                2,10,50,100,150 and 200 are given.
                -splitter:The strategy used to choose the split at each node. In this project, best and random are
                given.
                When we run it to find the best of these parameters in the Decision tree, a method called N-fold
                cross-validation is applied. In this method, N=5 was run. Some accuracy values ​​obtained when grid
                search is run are shared below.
        </p>
        <p>
                <center>
                        <img src="/static/img/table-3-2-1.jpeg" /><br>
                        Table 3.2.1 Grid Search Result of Decision Tree
                </center>

        </p>
        <p>
                At this point, we see that we get the best results without giving max_Depth and max_features. So we can
                plot the min_samples split and check if the value we actually get is the best. This graphic is shared in
                Figure 3.2.4.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-4.jpeg" /><br>
                        Figure 3.2.4 Min_samples_Split parameter Accuracy Result
                </center>

        </p>
        <p>
                According to this graph, we see that the entropy value gives better results than gini. Although a high
                value was found between 120 and 140 values, it continued with the same value for a certain time after
                140 and then suddenly decreased. Since this point is the elbow curve, 150 was chosen as the best result.
        </p>
        <p>
                As can be seen, the most optimal parameters were found. The result in our set with these parameters is
                given Figure 3.2.5:
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-5.jpeg" /><br>
                        Figure 3.2.5 Confusion Matrix and Result of Decision Tree
                </center>
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-6.jpeg" /><br>
                        Figure 3.2.6 Confusion Matrix as a Heatmap of Decision Tree
                </center>
        </p>
        <p>
                When this confusion matrix is ​​examined in figure 3.2.6, we see that the classes Acoustic(0) and
                7(Instrumental) are very similar to each other, and it is difficult to distinguish them from each other.
                However, it has been observed that the distinctions between other classes can be easily made.
        </p>
        <p>
                Finally, he drew a learning curve for the decision tree. Figure 3.2.7
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-7.jpeg" /><br>
                        Figure 3.2.7 Learning Curve of Decision Tree
                </center>
        </p>
        <p>
                In Figure 3.2.7,it is clearly seen that the model continued stably after 4000 sample size. Before that,
                the education process continued.
        </p>
        <br>
        <h6>Random Forest</h6>
        <br>
        <p>Random Forest works with different decision trees. For this reason, grid search should be applied. The
                parameters used in the random forest model to find the optimum results in this project are:
                -max_depth: The maximum depth of the tree. None tested as 10,50,100,150,200.
                -criterion: The function to measure the quality of a split. Two different values ​​were tried as Gini
                and Entropy.
                -n_estimators: The number of trees in the forest. 10, 50, 100, 150 and 200 values ​​were tried.
                -min_samples_split: The minimum number of samples required to split an internal node.2,10,50,100,150 and
                200 were tried.
        </p>
        <p>
                To find the optimum of these parameters, grid search was used again and optimum values ​​were searched
                with 5-fold cross validation. Some results are given in Table 3.2.2.
        </p>
        <p>
                <center>
                        <img src="/static/img/table-3-2-2.jpeg" /><br>
                        Table 3.2.2 Result of Random Forest
                </center>
        </p>
        <p>
                The optimum value was chosen as gini,50,150,50, respectively. The main reason for this is that we can
                find the same accuracy value by applying less operations. The accuracy and confusion matrix of these
                results is given in Figure 3.2.8.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-8.jpeg" /><br>
                        Figure 3.2.8 Accuracy of Random Forest
                </center>
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-9.jpeg" /><br>
                        Figure 3.2.9 Confusion matrix Heatmap of Random Forest
                </center>

        </p>
        <p>
                When this confusion matrix is ​​examined in figure 3.2.9 , we see that the classes Acoustic(0) and
                7(Instrumental) are very similar to each other, and it is difficult to distinguish them from each other.
                alternative music and instrumental were difficult to separate in the Decision Tree model. Random Forest
                did this better than Decision Tree.
        </p>
        <br>
        <h6>CatBoost</h6>
        <br>
        <p>Parameters to be tested in Catboost:
                -depth:Depth of a tree. The numbers between 4 and 10 are given as ranges.
                -learning_rate:Step size shrinkage used in update to prevents overfitting. The range is between 0.01 and
                0.04.
                -iterations:Max count of trees.100,500,1000,1200 and 1500 are given.
                Catboost ran longer than Random Forest and Decision Tree. For this reason, the optimum result appears in
                figure 3.2.10.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-10.jpeg" /><br>
                        Figure 3.2.10 Best Parameter for CatBoost
                </center>
        </p>
        <p>
                However, it was seen that 0.86 accuracy value, which is a higher result than the other two tree
                algorithms, was obtained.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-11.jpeg" /><br>
                        Figure 3.2.11 Confusion matrix Heatmap of CatBoost
                </center>
        </p>
        <p>
                In Figure 3.2.11, it is seen that it is better in separating Acoustic(0) and 7(Instrumental) compared to
                the other two tree algorithms.

        </p>
        <br>
        <h6>AdaBoost</h6>
        <br>
        <p>Several different models were created in Adaboost and various accuracies were calculated for these models.
                The list of models created is shown in Figure 3.2.12. The result created accordingly is seen in Figure
                3.2.13.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-12.jpeg" /><br>
                        Figure 3.2.12 Different models for AdaBoost
                </center>
        </p>
        <p>
                Some parameters in adaboost are: n_estimators and learning rate. n_estimators decide the number of
                trees. In this project only 50,60 and 70. learning rate is given 0.1 and 2.0. It changes how fast
                algorithms learn.
        </p>
        <p>
                <center>
                        <img src="/static/img/figure-3-2-13.jpeg" /><br>
                        Figure 3.2.13 AdaBoost Result
                </center>
        </p>
        <p>
                The best result of the Adaboost parameter was 0.84.
        </p>
        <br>
        <hr>
</div>
{% endblock %}